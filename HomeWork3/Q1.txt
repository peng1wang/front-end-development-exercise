为什么：
在JavaScript中的二进制的浮点数0.1和0.2并不是十分精确，
JavaScript遵循IEEE754标准，在64位中存储一个数据的有效数字形式。
其中，第0位为符号位，0表示正数1表示负数；第1到11位存储指数部分；
第12到63位存小数部分（尾数部分）（即有效数字）。
由于二进制的有效数字总是表示为 1.xxx…的形式，尾数部分在规约形式下的第一位默认为1，
故存储时第一位省略不写，尾数部分f存储有效数字小数点后的xxx...，最长52位。
因此，JavaScript提供的有效数字最长为53个二进制位（尾数部分52位+被省略的1位）。
对于52位之后进行舍入运算，此时可看作0舍1入（具体舍入规则在第三部分详细说明），有精度损失。
0.1与 0.2相加的结果并非正好等于0.3，而是一个比较接近的数字 0.30000000000000004。
解决方案：
设置一个误差范围值，通常称为”机器精度“，而对于 JavaScript 来说，这个值通常是2^-52,
而在 ES6 中，已经为我们提供了这样一个属性：Number.EPSILON，而这个值正等于2^-52。这个值非常非常小，
在底层计算机已经帮我们运算好，并且无限接近0，但不等于0,。
这个时候我们只要判断(0.1+0.2)-0.3小于Number.EPSILON，
在这个误差的范围内就可以判定0.1+0.2===0.3为true。